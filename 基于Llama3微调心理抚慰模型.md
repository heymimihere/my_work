# 基于Llama3微调心理抚慰模型

## 一、概述

本对话系统基于Llama3预训练模型，并基于LORA进行指令监督微调（SFT）使其专注于心理抚慰领域。系统通过transformers库加载Llama3模型和tokenizer，并使用peft库加载并合并LORA微调模型。用户通过文本输入进行交互，该对话系统可以根据用户的输入生成相应的心理抚慰建议。



## 二、微调工具

llama-factory是一个易于使用的大规模语言模型微调框架，它支持很多种开源大语言模型且支持多种训练方法。llama-factory的使用形式分为指令与界面两种，允许用户通过内置的Web UI灵活定制微调，而无需编写代码。通过配置文件设置一系列训练参数，使用llama-factory启动微调过程。在微调过程中，llama-factory将自动处理模型的前向传播、反向传播、权重更新等步骤。
![img](https://github.com/heymimihere/my_work/blob/main/data/images/image-1.png)

## 三、模型结构

### 1.预训练模型

本对话系统采用Llama3作为预训练模型，该模型是一个大型的自回归语言模型，具有强大的文本生成能力。Llama3模型通过在大规模的文本数据上进行训练，学习了丰富的语言知识和上下文理解能力。

### 2.准备数据

为了进行微调，需要准备与心理抚慰相关的数据集。使用自定义数据集，添加数据集描述后，并将数据集修改为相应的格式。

### 3.LORA微调模型

为了使Llama3模型更好地适应心理抚慰领域，使用了LORA微调技术。LORA微调通过在基础模型上添加低秩矩阵，对模型进行微调，从而实现对特定任务的优化。该模型微调过程中使用bash脚本调用Python训练脚本，并在指定的GPU上微调预训练的Llama模型。训练数据为data_pro_2数据集，在每个设备上设置批次大小为1，采用余弦学习率调度器，并累积8个步骤的梯度进行更新。该过程每100步保存模型、评估模型性能，并在训练结束时加载性能最佳的模型。最后，微调后的模型将保存在指定的输出目录中。
微调模型时，调整了多个参数以适应该训练任务以及数据集。其中，学习率被设置为5e-5，这通常是一个较小的值，防止对预训练模型造成过大影响。动态地调整学习率，确保模型在训练过程中能够更好地收敛。梯度累积步数被设定为8，使得模型在更新参数之前累积多个小批次的梯度，从而在不增加内存负担的情况下模拟较大的批次大小。训练轮数被设定为3.0，模型将遍历整个数据集三次。限制所使用的样本数最大为3000个，增加最大样本数可能会增加计算资源的使用和训练时间，而减少样本数可能会加速训练但可能降低模型的性能。在调整这个参数时，需要根据计算资源以及预期的模型性能来进行指定。

### 4.Tokenizer

Tokenizer用于将文本输入转换为模型可以理解的token IDs，使用与Llama3模型相匹配的tokenizer，确保输入与模型的兼容性。

### 5.对话循环

系统通过对话循环与用户进行交互。在每次循环中，首先等待用户输入，并将其添加到对话历史中,将对话历史转换为输入token IDs，并传递给模型进行文本生成。模型根据输入token IDs生成响应token IDs，系统将其解码为文本并返回给用户。

## 四、功能特点

通过LORA微调技术，系统能够专注于心理抚慰领域，生成专业的心理抚慰建议。系统采用对话形式与用户进行交互，能够根据用户的输入动态生成响应，实现更加自然且流畅的对话体验。

## 五、验证和测试

将整个数据集划分为训练集和验证集，其中验证集占总数据集的10%。训练后在验证集上验证模型，以检查模型的泛化能力。在验证阶段，在每个训练周期结束后，记录并输出训练集上的损失、验证集上的损失。这些指标会被记录并输出。

![img](https://github.com/heymimihere/my_work/blob/main/data/images/image-2.jpg)![img](https://github.com/heymimihere/my_work/blob/main/data/images/image-3.jpg) 

## 六、使用实例

以下是一个简单的使用示例，展示了用户与系统之间的对话流程。

![img](https://github.com/heymimihere/my_work/blob/main/data/images/image-4.jpg)

![img](https://github.com/heymimihere/my_work/blob/main/data/images/image-5.jpg)

 
